employee.json:
[
    {"Age": "21", "id": "201","name": "Durgesh",},
    {"Age": "26", "id": "202","name": "Calvin"},
    {"Age": "10", "id": "203","name": "Apurva"},
    {"Age": "16", "id": "204","name": "Saail", }
]

Reading the json document and show the data
>>val sqlcontext = new org.apache.spark.sql.SQLContext(sc)
>>val dfs = sqlcontext.read.json("employee.json")
>>dfs.show()
>>dfs.printSchema()
>>dfs.select("name").show()

Filter: Finding employee whose age us greater than 23
>>dfs.filter(dfs("age") > 23).show()

Groupby method: Count the number of employees who are of same age
>>dfs.groupBy("age").count().show()


SQL QUERY
Create sqlContext and import sql functions
>> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
>> import sqlContext.implicits

Creating schema of employee data using case class
>>case class Employee(id: Int, name: String, age: Int)

Create rdd and apply transformations
>>val empl = sc.textFile("employee.txt")
  .map(_.split(","))
  .map(e => Employee(e(0).trim.toInt, e(1), e(2).trim.toInt))
  .toDF()

Store the dataframe in a table:
>>empl.registerTempTable("employee")

Select query on dataframe:
>>val records = sqlContext.sql("SELECT * FROM employee")
>>records.show()

Using where clause: Age between 20 and 35
>>val agefilter = sqlContext.sql("SELECT * FROM employee WHERE age >= 20 AND age <= 35")
>>agefilter.show()

ForEach
>>gefilter.map(t => "ID: " + t(0)).collect().foreach(println)



