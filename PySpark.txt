>>pyspark
>>input = spark.read.text("README.md")  //to read the readme file
>>input.count()    //read number of rows
>>input.first()    //read first row
>>lines = input.filter(input.value.contains("Spark"))    //lines that only contains spark word
>>input.filter(input.value.contains("Spark")).count()   //show lines with specific word: spark

Counts the maximum number of words per line in the whole file:
>>from pyspark.sql.functions import *
>>input.select(size(split(input.value,"\s+")).name("numWords")).agg(max(col("numWords"))).collect()

wordcount:
>>wordCount=input.select(explode(split(input.value,"\s+")).alias("word")).groupBy("word").count()
>>wordcount.collect()


Simple spark applications

App.py:
from pyspark.sql import SparkSession
logFile = "README.md"
spark = SparkSession.builder.appName("SimpleApp").getOrCreate()
logData = spark.read.text(logFile).cache()
numAs = logData.filter(logData.value.contains('a')).count()
numBs = logData.filter(logData.value.contains('b')).count()
print(f"Lines with 'a': {numAs}, lines with 'b': {numBs}")
spark.stop()

>>python app.py
